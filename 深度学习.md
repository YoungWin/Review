
author:young<br>
深度学习<br>

## 损失函数
（1.）你知道有哪些损失函数？
https://zhuanlan.zhihu.com/p/47202768
0-1损失：如果预测值与目标值不相等，那么为1，否则为0。
感知损失：如果预测值与目标值相差小于某阈值，值为1，否则为0。
Hinge Loss：解决间隔最大化问题，如在SVM中解决几何间隔最大化问题。
交叉熵损失：LR
平方损失：用于回归问题
绝对值损失：用于回归问题
指数损失：Adaboost
正则


## 激活函数
（1.）sigmoid，tanh，Relu等激活函数的优缺点（这里之前没有复习到，后面想了想应该从梯度消失，数据压缩，0均值方面来解释；当然，说的越多越好，比如Relu的神经元dead啥的，越能体现你的知识广度）

（2.）有哪些激活函数，各自有什么优点，具体做模型的时候怎么选 那无非就是0均值 梯度消失梯度爆炸 可导啥的

（3.）sigmoid和softmax的区别，sigmoid判别阈值如何选？

（4.）softmax 与 二分类 比有什么特点

（5.）非线性激活函数
https://www.bilibili.com/video/BV1i4411v7H3
退化成线性模型，模型复杂度过低，无法拟合复杂数据集



## 优化器
（1.）常用的优化算法有哪些？各自的优缺点？有了解哪些优化方法。（讲了SGD, 到一阶动量、二阶动量，adam，nadam， 以及adam不收敛的问题等等）

（2.）说说Adam等优化器的区别（从Momentum到RMSprop到Adam以及Adam可能不会收敛，还说了NAG和AdaGrad），adam的默认参数有哪些

（3.）adam公式写一下（同时介绍了adam是momentum和RMSprop的结合）

（4.）为什么 adagrad 简单求和不好？

（5.）说下adam的思想





## 梯度
（1.）梯度消失产生原因、解决方法

（2.）梯度爆炸产生原因、解决方法


## 初始化
（1.）神经网络中网络权重W初始化为0有什么问题？


（2.）你对权值初始化有什么了解？怎样才算是好的初始化？

（3.）如何提高训练速度
https://www.bilibili.com/video/BV1j441187FF
对输入数据 处理成 N(0,1)
搜索轮廓是一个圆形，无论从哪个方向训练，梯度下降速度差不多快
搜索轮廓如果是椭圆形，选的起点不是很好时，下降就会很慢


（4.）说下平时用到的深度学习的trick

































