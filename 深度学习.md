
author:young<br>
深度学习<br>

## 损失函数
（1.）你知道有哪些损失函数？<br>
https://zhuanlan.zhihu.com/p/47202768
0-1损失：如果预测值与目标值不相等，那么为1，否则为0。
感知损失：如果预测值与目标值相差小于某阈值，值为1，否则为0。
Hinge Loss：解决间隔最大化问题，如在SVM中解决几何间隔最大化问题。
交叉熵损失：LR
平方损失：用于回归问题
绝对值损失：用于回归问题
指数损失：Adaboost
正则


## 激活函数
（1.）sigmoid，tanh，Relu等激活函数的优缺点（这里之前没有复习到，后面想了想应该从梯度消失，数据压缩，0均值方面来解释；当然，说的越多越好，比如Relu的神经元dead啥的，越能体现你的知识广度）<br>

（2.）有哪些激活函数，各自有什么优点，具体做模型的时候怎么选 那无非就是0均值 梯度消失梯度爆炸 可导啥的<br>

（3.）sigmoid和softmax的区别，sigmoid判别阈值如何选？<br>

（4.）softmax 与 二分类 比有什么特点<br>

（5.）非线性激活函数<br>
https://www.bilibili.com/video/BV1i4411v7H3<br>
退化成线性模型，模型复杂度过低，无法拟合复杂数据集<br>



## 优化器
（1.）常用的优化算法有哪些？各自的优缺点？有了解哪些优化方法。（讲了SGD, 到一阶动量、二阶动量，adam，nadam， 以及adam不收敛的问题等等）<br>

（2.）说说Adam等优化器的区别（从Momentum到RMSprop到Adam以及Adam可能不会收敛，还说了NAG和AdaGrad），adam的默认参数有哪些<br>

（3.）adam公式写一下（同时介绍了adam是momentum和RMSprop的结合）<br>

（4.）为什么 adagrad 简单求和不好？<br>

（5.）说下adam的思想<br>
同时利用一阶梯度和二阶梯度：a. 前i-1项一阶梯度和 + 第i项一阶梯度，b. 前i-1项二阶梯度和 + 第i项二阶梯度，c. 梯度更新一阶梯度和/sqrt(二阶梯度和)，d. 参数theta_i+1=theta_i+梯度更新<br>




## 梯度
（1.）梯度消失产生原因、解决方法<br>

（2.）梯度爆炸产生原因、解决方法<br>


## 初始化
（1.）神经网络中网络权重W初始化为0有什么问题？<br>


（2.）你对权值初始化有什么了解？怎样才算是好的初始化？<br>

（3.）如何提高训练速度<br>
https://www.bilibili.com/video/BV1j441187FF<br>
对输入数据 处理成 N(0,1)<br>
搜索轮廓是一个圆形，无论从哪个方向训练，梯度下降速度差不多快<br>
搜索轮廓如果是椭圆形，选的起点不是很好时，下降就会很慢<br>


（4.）说下平时用到的深度学习的trick<br>

































