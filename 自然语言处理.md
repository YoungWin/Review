
author:young
自然语言处理算法细节


### fasttext
###### 文本分类fasttext



###### 词向量嵌入CBOW&Skip-Gram






### Attention机制
https://www.bilibili.com/video/BV1L4411q785?p=1
All attention is used

（1.）引入attention的目的是什么？
输入句子过长，造成遗忘的现象；解决办法是存储输入序列各个词的编码结果

（2.）seq2seq模型
每一次翻译一个词时，整个句子encoder完，decoder序列当前词前面的词
a.长序列遗忘问题
b.对齐问题[encoder完 压缩了输入序列]

（3.）翻译模型的学习过程
a.把encoder过程 每个词学习到隐层向量$h_i$，存下来
b.decoder词向量=encoder部分词向量 加权求和**********

注意：
@ 每个词encoder的方式可以使lstm/gru等
@ 计算decoder某个词$y_i$词向量：$y_i=f(c_{背景词}，y_{i-1},...,y_0)$
其中，$c_{背景词}=sum(权重i \dot hi)$
@ 权重：根据 qt与hi的语义相似度 归一化 确定
对于qt的选取
（1）找yi时刻，对应取前一时刻的$qt=yi-1$ 
（2）找当前时刻$qt=hi$


相似度计算：（1）内积，yi-1和hi的维度一致，decoder和encoder向量维度不一致
[加一层网络变换x^T*Wy(更好关联了x和y)；x和y做concat，利用一个隐层网络，再预测结果概率/相似度 v^T*tanh(W[x,y])] 
（2）


