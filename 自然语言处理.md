
author:young<br>
自然语言处理算法细节<br>


## fasttext
#### 文本分类fasttext



#### 词向量嵌入CBOW&Skip-Gram






## Attention机制
https://www.bilibili.com/video/BV1L4411q785?p=1<br>
https://zhuanlan.zhihu.com/p/37601161<br>
All attention is used<br>

（1.）引入attention的目的是什么？<br>
输入句子过长，造成遗忘的现象。解决办法是 存储 输入序列各个词的编码结果。<br>

（2.）seq2seq模型<br>
每一次翻译一个词时，整个句子encoder完，decoder序列当前词前面的词<br>
a.长序列遗忘问题；b.对齐问题(encoder完 压缩了输入序列)<br>

（3.）翻译模型的学习过程
STEP     | 操作
-------- | -----
1  | encoder过程 每个词学习到隐层向量h_i 存下来(每个词encoder的方式用lstm/gru等)
2  | decoder词向量=encoder隐层向量h_i 加权求和**

STEP     | 操作
-------- | -----
计算decoder某个词y_i词向量  | y_i = f(c_{背景词}，y_{i-1},...,y_0)
c_{背景词}  | sum(权重i * encoder隐层向量h_i)
权重i  | decoder t时刻得分q_t 与 encoder隐层向量h_i 的语义相似度，归一化
decoder t时刻得分q_t  | (1)找y_i时刻，对应取前一时刻的q_t=y_{i-1}；(2)找当前时刻q_t=h_i
语义相似度  | (1)内积，yi-1和hi的维度一致，decoder和encoder向量维度不一致



加一层网络变换x^T*Wy(更好关联了x和y)；x和y做concat，利用一个隐层网络，再预测结果概率/相似度 v^T*tanh(W[x,y])]
（


