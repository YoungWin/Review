
author:young<br>
自然语言处理算法细节<br>

## 朴素贝叶斯
（1.）讲解一下朴素贝叶斯的底层原理，比如说，如何选参数，如何训练模型，如何做分类？<br>

（2.）稀疏贝叶斯是什么样的算法？这个算法有什么样的应用领域？分类算法中哪些是线性的？决策树是不是线性的？<br>


## tf-idf  
tf：一个词在一个文档里的频次/该文档里全部词数<br>
idf：log(文档总数/包含这个词的文档树)<br>


## fasttext
#### 词表的构建
**一、ReadVocab()方法，指定词库(默认使用这种方法)**<br>
**a.) 对文本进行处理(低频词、哈希表)**<br>
在词库里增加"</s>"表示序列的开始，同时读取文本时，处理换行符，将换行符转化成"</s>"<br>
读取文件里的每一个词，并在词库里查找，若存在该词，对应该词的词频+1；如不存在，则在词库里添加该词(这里是核心，词频计数是为了构建哈弗曼树)<br>
在词库里，通过hash表的方式进行存储，最终会过滤低频词<br>
最后，还需要根据词库里的词频，对词库里的词从大到小排序<br>
**b.) 对词的哈希处理**<br>
保留两个数组：存储词的vocab、存储词的hash的vocab_hash<br>
1.存储词的结构体————包括词频、从根节点到叶子节点的路径、对词进行哈夫曼编码、编码长度<br>
2.vocab_hash存储的是词在词库里的index<br>
对词的处理包括：计算词的hash值、检索的词是否存在(存在返回词在词库里的索引，不存在返回-1)<br>
**c.) 对低频词的处理**<br>
vocab_size当前以及构建好的词库包含词的个数<br>
vocab_hash_size初始设定的hash表的大小<br>
当vocab_size>0.7 · vocab_hash_size时，需要对低频次处理<br>
如果词频小于min_reduce时，从词库里删除该词<br>
删除低频词后，对词库里的词重新进行hash计算<br>
**d.) 每一轮训练，都根据更新后的词库进行训练**<br>

**二、LearnVocabFromTrainFile()从词的文本构建词库**<br>

#### 训练细节(google code开源C++版本)
网络初始化：1.词向量初始化；2.映射层到输出层权重初始化：hs、negative sampling<br>
训练过程利用fseek实现多线程，每个线程分配指定大小的文件<br>

#### 文本分类fasttext
一条分类样本，分词、查词表、embedding、一个样本的各个词avg，sigmoid做二分类预测，损失函数交叉熵<br>


#### 词向量嵌入CBOW
输入层：中心词和它对应的上下文词<br>
隐层：把中心词和它的上下文词(词向量找到)，求和取平均<br>
输出层：词向量构建softmax之后，对应的权重<br>


#### 词向量嵌入Skip-Gram

#### negative sampling

#### 介绍层次Softmax
哈夫曼树的构建：初始化定义33个长度为vocab_size · 2+1的数组：前vocab_size位置 存储的是每个词对应的词频，后面位置存储的是很大的数<br>

hs在树里，给高频词分配了很短路径，给低频次分配了一个较深的路径<br>

计算sigmoid-近似计算：固定区间划分若干等分，区间里的值 提前算好 存入数组<br>







## Attention机制
https://www.bilibili.com/video/BV1L4411q785?p=1<br>
https://zhuanlan.zhihu.com/p/37601161<br>
All attention is used<br>

（1.）引入attention的目的是什么？<br>
输入句子过长，造成遗忘的现象。解决办法是 存储 输入序列各个词的编码结果。<br>

（2.）seq2seq模型<br>
每一次翻译一个词时，整个句子encoder完，decoder序列当前词前面的词<br>
a.长序列遗忘问题；b.对齐问题(encoder完 压缩了输入序列)<br>

（3.）翻译模型的学习过程
STEP     | 操作
-------- | -----
1  | encoder过程 每个词学习到隐层向量h_i 存下来(每个词encoder的方式用lstm/gru等)
2  | decoder词向量=encoder隐层向量h_i 加权求和**

STEP     | 操作
-------- | -----
计算decoder某个词y_i词向量  | y_i = f(**c_{语义向量}**，y_{i-1})
c_{语义向量}  | sumall(**权重i** * encoder隐层向量h_i)
权重i  | **decoder t时刻查询向量q_t** 与 各个encoder隐层向量h_i 的**语义相似度**，归一化
decoder t时刻查询向量q_t  | 若计算y_i：(1)对应取q_t=y_{i-1}；(2)找当前时刻q_t=h_i
语义相似度  | (1)**内积，y_{i-1}和h_i的维度一致**，decoder和encoder向量维度不一致
内积，y_{i-1}和h_i的维度不一致  | (1)加一层网络变换x^T Wy(更好关联了x和y)；（2）x,y concat，利用一个隐层网络，预测结果概率看成相似度 v^T tanh(W(x,y))(v是最后一层参数)



##### 计算语义相似度时，选择的attention机制类型
attention类型     | 特征
-------- | -----
soft-attention  | 用 各个encoder隐层向量h_i，根据相似度 加权求和
hard-attention  | 只用某些词的隐层向量h_i(强化学习，不可微较困难)



（1.）什么是self-attention，什么情况下要用，K、Q、V分别是啥？<br>
（2.）attention-model<br>
把此时模型的点集中在离它更近的部分，而不是管离它较远的sequence<br>





## LSTM&RNN
（1.）LSTM原理解释，每个门用什么激活函数，我真有点忘了说遗忘门是sigmoid 输入输出是tanh 他说没有问题 我也不知道是不是真的没有问题<br>


（2.）LSTM与RNN的区别<br>


（3.）LSTM怎么缓解梯度消失<br>


























