
author:young<br>
树模型<br>

## 决策树
https://zhuanlan.zhihu.com/p/85731206<br>

（1.）CART了解吗？怎么做回归和分类的？<br>
CART是分类回归树，是一个二叉树。在回归树中是找到一个特征分界点 使得预测值方差最小，而在分类树当中也是找到一个特征分界点 使得gini指数最小。<br>

（2.）树模型中分叉的判断有哪些：信息增益，信息增益比，Gini系数；它们有什么区别？<br>
度量指标     | 原理
-------- | -----
信息增益  | 数据集各个类别分布的熵 - 以A为条件分组，各组内类别分布的熵
信息增益比  | 信息增益 / A各个取值分布的熵
Gini系数  | 根据特征A选取切分点，切分两组样本里 各个类别概率pk(1-pk)求和
基尼指数反映了**从数据集中随机抽取两个样本，其类别标记不一致的概率**。因此基尼指数越小，则数据集纯度越高。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0-1 之间的数，0 是完全相等，1 是完全不相等。<br>
lnx=-1+x+o(x)，基尼指数可以理解为熵模型的一阶泰勒展开。<br>


（3.）CART树和C4.5对比<br>
CART     | C4.5(分类)
-------- | -----
分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，**CART 没有停止准则，会一直生长下去**  | 分裂：先从候选划分特征中 找到**信息增益 高于平均值的特征，再从中选择 增益率最高的**
剪枝：**后剪枝**，递归地从低往上 针对每一个非叶子节点 评估**用一个最佳叶子节点 去代替这棵子树 是否有益**。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。  | 剪枝：**后剪枝-代价复杂度剪枝**，从最大树开始，每次选择 熵对整体性能贡献最小的那个分裂节点 作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树



（4.）C4.5如何处理缺失值的？<br>
问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）
问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里） 针对问题一：对于具有缺失值特征，**用 没有缺失的样本子集 所占比重 来折算**； 
针对问题二：**将样本 同时划分到所有子节点**，不过要调整样本的权重值，其实也就是 以不同概率 划分到不同节点中。


## 随机森林、GBDT、XGBoost
（1.）随机森林的随机怎么理解？<br>



（2.）比较下随机森林和GBDT<br>


（3.）GDBT的基模型是什么，对应的损失函数是什么，怎么分裂的节点<br>


（4.）GDBT的复杂度<br>


（5.）GBDT可以并行吗？<br>


（6.）GBDT与XGboost的区别、优劣点。<br>


（7.）xgboost手推<br>



（8.）xgboost怎么处理洗漱类别型特征？<br>




（9.）LightGBM和xgboost的区别<br>



（10.）LightGBM的直方图排序后会比xgboost的效果差吗，为什么？<br>









