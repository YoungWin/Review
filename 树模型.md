
author:young<br>
树模型<br>

## 决策树
https://zhuanlan.zhihu.com/p/85731206<br>

（1.）CART了解吗？怎么做回归和分类的？<br>
CART是分类回归树，是一个二叉树。在回归树中是找到一个特征分界点 使得预测值方差最小，而在分类树当中也是找到一个特征分界点 使得gini指数最小。<br>

（2.）树模型中分叉的判断有哪些：信息增益，信息增益比，Gini系数；它们有什么区别？<br>
度量指标     | 原理
-------- | -----
信息增益  | 数据集各个类别分布的熵 - 以A为条件分组，各组内类别分布的熵
信息增益比  | 信息增益 / A各个取值分布的熵
Gini系数  | 根据特征A选取切分点，切分两组样本里 各个类别概率pk(1-pk)求和

基尼指数反映了**从数据集中随机抽取两个样本，其类别标记不一致的概率**。因此基尼指数越小，则数据集纯度越高。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0-1 之间的数，0 是完全相等，1 是完全不相等。<br>
lnx=-1+x+o(x)，基尼指数可以理解为熵模型的一阶泰勒展开。<br>


（3.）CART树和C4.5对比<br>
CART     | C4.5(分类)
-------- | -----
分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，**CART 没有停止准则，会一直生长下去**  | 分裂：先从候选划分特征中 找到**信息增益 高于平均值的特征，再从中选择 增益率最高的**
剪枝：**后剪枝**，递归地从低往上 针对每一个非叶子节点 评估**用一个最佳叶子节点 去代替这棵子树 是否有益**。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。  | 剪枝：**后剪枝-代价复杂度剪枝**，从最大树开始，每次选择 熵对整体性能贡献最小的那个分裂节点 作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树

从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征。<br>


（4.）C4.5如何处理缺失值的？<br>
缺失情形     | 解决办法
-------- | -----
在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）  | 对于具有缺失值特征，**用 没有缺失的样本子集 所占比重 来折算**
选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里）  | **将样本 同时划分到所有子节点**，不过要调整样本的权重值，其实也就是 以不同概率 划分到不同节点中


## 随机森林、GBDT、XGBoost
https://zhuanlan.zhihu.com/p/139381538<br>

（1.）随机森林的随机怎么理解？<br>
样本随机、特征随机<br>

（2.）比较下随机森林和GBDT<br>
GBDT和随机森林的相同点：1、都是由多棵树组成；2、最终的结果都是由多棵树一起决定<br>
GBDT和随机森林的不同点：<br>
1、组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成<br>
2、组成随机森林的树可以并行生成；而GBDT只能是串行生成<br>
3、对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将**所有结果累加**起来，或者加权累加起来<br>
4、随机森林对异常值不敏感，GBDT对异常值非常敏感<br>
5、随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成<br>
6、随机森林是通过**减少模型方差**提高性能，GBDT是通过**减少模型偏差**提高性能<br>

（3.）GDBT的基模型是什么，对应的损失函数是什么，怎么分裂的节点<br>
https://www.bilibili.com/video/BV157411R7eh?p=2<br>
GBDT是提升树模型，是基于前向分布算法的。GBDT基模型是CART回归树。**损失函数是Loss=L(yi, fi-1 + a T(theta))**。每一次根据**损失函数在当前模型的负梯度**学习一棵回归树。回归树分裂根据mse。<br>
GBDT公式Loss进行一阶泰勒展开，T(theta)取L关于fi-1的负梯度，出现负的平方项-确保损失函数越来越小。<br>

（4.）GDBT的复杂度<br>
求损失函数的负梯度(在每个样本点)；根据负梯度拟合回归树。~~~ <br>

（5.）GBDT可以并行吗？描述GBDT算法流程<br>
不可以。boosting串行生成树，后一棵树需要根据前面生成树的结果，求损失函数的负梯度，拟合负梯度生成回归树。通过牛顿迭代法 计算出新树的学习率系数。<br>

（6.）GBDT与XGBoost的区别、优劣点。<br>
GBDT：对损失函数一阶泰勒展开，对负梯度拟合回归树；<br>
XGBoost：对损失函数二阶泰勒展开，对叶子节点添加正则化项；分裂直接根据损失增益，速度更快。<br>

（7.）XGBoost手推<br>
损失函数二阶泰勒展开；每个样本点f(xi)转化成按叶子节点分组损失；和正则项合并；求最优叶子节点权重、反带回损失函数；-G^2/H+lambda + gamma<br>

（8.）XGBoost怎么处理稀疏类别型特征？<br>
二分类可以直接映射成数值使用，多分类稀疏特征一般不建议onehot分裂成很多特征列：一方面降低算法速度；一方面每个特征列非零项很少，单独考查增益很小，一般难以满足分裂最低增益要求。deep embedding forest(把xgboost在深度或广度进行stack)、gcforest、sGBM(解决树模型不可微，用一种软决策树的可微基学习器按xgboost的思想构建模型)<br>


（9.）LightGBM和XGBoost的区别<br>
https://www.bilibili.com/video/BV1vJ41157az?from=search&seid=17713386359445395940<br>


（10.）LightGBM的直方图排序后会比XGBoost的效果差吗，为什么？<br>









