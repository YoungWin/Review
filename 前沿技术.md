## 知识蒸馏Network Compression
https://www.bilibili.com/video/BV1SC4y1h7HB?from=search&seid=9439205011855570419<br>
希望把deep model放到 有限空间资源的移动设备 上，把network变小的技术：（1）Network Pruning（2）Knowledge Distillation（3）Parameter Quantization（4）Architecture Design（5）Dynamic Computation<br>
【1.】为什么需要Network Pruning？<br>
小的Network更难训练，有很多paper证明：只要Network足够大，可以用gradient descent找到全局最小。<br>
大乐透假设：用大Network的参数初始化小Network相应结构的参数，就可以把小的Network训练起来。(如果完全初始化小Network，小Network是无法训练起来的)<br>
【2.】Knowledge Distillation什么意思？<br>
（1）可以先train一个大的Network，再train一个小的Network学习大的Network的行为(小模型的预测概率和大模型的预测概率 计算交叉熵损失)。<br>
（2）大模型提供了比label更多的信息，甚至可以说训练小Network时 未提供的的样本，可以通过大Network的预测结果学习到。<br>
（3）对多个ensemble结果，合并起来各个结果，学习一个小Network，便于线上使用。<br>
（4）对softmax的输入除以T，缩减不同类别输入值差异，拉近预测概率，用于区别和真实值只有一个label=1的分布。————不一定特别有用<br>


## 胶囊网络Capsule Network
https://www.bilibili.com/video/BV1eW411Q7CE?from=search&seid=15018787840603176667<br>
capsule主要是想取代neuron：capsule输出是vector，neuron输出是value。<br>
每一个neuron的任务是检测 一种特定的模式，capsule的vector代表了 某一种类模式。<br>

LAYER     | 操作
-------- | -----
u1=W1v1，u2=W2v2  | Wi是bp过程学习出来的(ui和vi都是vector)
s=c1u1+c2u2  | ci理解成pooling-根据上层输出决定(如果v和u很相近，下一次迭代的ci会增加)
v=Squash(s)  | v=|s|^2/(1+|s|^2)·s/|s|，Squash挤压只会改变s的长度、而不会改变s的方向

### Dynamic Routing
ci的确定过程有点类似排除异常点的过程，算出的v与哪些ui更接近，下一次迭代ci就会更倾向给哪些ui更大权重。<br>
用上一轮计算好的v和ui的相似度，去更新ci；利用更新后的ci去计算下一轮。过程类似RNN。<br>
输出层：一排vector，比如手写数字识别，输出成10个10维度向量，每个10维度向量都是预测该数字概率。(联想skip-gram的输出层图)<br>




