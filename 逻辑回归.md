
author:young<br>
逻辑回归模型<br>

https://zhuanlan.zhihu.com/p/74874291<br>
（1.）详细介绍一下逻辑回归，包括逻辑回归的分类公式，损失函数、逻辑回归分类的过程。<br>
假设样本分布是二项逻辑斯蒂分布，P(Y=1|x)=sigmoid(wx)，交叉熵损失-yilogpi-(1-yi)log(1-pi)。特征进行线性组合，然后做sigmoid变换映射成概率，概率>0.5正样本。<br>


（2.）具体讲解一下线性回归的底层原理，比若说如何训练，如何得到参数，如何调整参数等。<br>
对交叉熵损失函数求导，通过SGD更新梯度：损失函数关于线性组合系数wi的导数为gi=(p(xi)-yi)·xi，其中p(xi)就是sigmoid(wx)预测的概率，对于第i个维度 wk+1=wk-a·gi。<br>


（3.）线性回归和逻辑回归的区别和联系，从广义线性模型角度考虑。优化方面？<br>
https://zhuanlan.zhihu.com/p/35250134<br>
逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，变成了分类算法。本质上，两者都属于广义线性模型，都是从指数分布族导出的线性模型，线性回归假设Y|X服从高斯分布，逻辑回归假设Y|X服从伯努利分布。<br>
逻辑回归是分类模型，输出的是离散值；线性回归是回归模型，输出的连续值。线性回归是在实数域范围内进行预测，而分类范围则需要在[0,1]，逻辑回归减少了预测范围；线性回归在实数域上敏感度一致，而逻辑回归在 0 附近敏感，在远离 0 点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。<br>


（4.）逻辑回归中损失函数的作用？逻辑回归中 除了损失函数能衡量模型的好坏，还有没有其他的方法？<br>



（5.）逻辑回归输出的0-1之间的值，是概率值吗？这个值又叫对数几率回归，怎么理解几率这个概念？<br>
概率 y 视为 x 为正例的概率，则 1-y 为 x 为其反例的概率————两者的比值称为几率（odds），指**该事件发生与不发生的概率比值**。<br>


（6.）logit函数和sigmoid函数的关系<br>
logit(p)=log(p/(1-p))=wx。<br>


（7.）权值初始化方式对LR的收敛有影响吗？<br>
https://zhuanlan.zhihu.com/p/68048448<br>
tf.random_normal方差从1减小到0.1收敛性变好、震荡降低。在不使用BN的深度学习网络中，梯度经常会消失或者爆炸。一套网络的权重最好不要相差太大。如果在网络初始化的时候，权值的初始化方差太大，会导致网络输出增大并导致梯度饱和/消失。而且通常情况下不会收敛到一个比坏较的值上。<br>
如果初始的方差小，如0.1，会导致在前向传播过程中，不同的层的输入不断减小。这会导致很多问题，因为权重的梯度跟输入息息相关。那这样子会导致权重的更新速度很慢很慢。初始化的**方差如果太大**，就会使得每一层的输出越来越大。形如tanh激活函数，就会容易导致**梯度饱和**的现象。<br>


（8.）对于LR来说，LR如果多了一维冗余特征，其权重和AUC会怎样变化？<br>
权重变为1/2， AUC不会变化。<br>


（9.）LR为什么更容易并行<br>
逻辑回归的并行化最主要的就是**对目标函数 梯度计算的并行化**。目标函数的梯度向量计算中只需要进行向量间的点乘和相加，可以很容易将每个迭代过程拆分成相互独立的计算步骤，由不同的节点进行独立计算，然后归并计算结果。<br>


（10.）LR为什么不可以用MSE作为损失函数？<br>
MSE 会有梯度消失现象(sigmoid函数求导，在0和1两端惹的祸)，MSE 的导数非凸 求最优解困难。<br>


（11.）怎么理解最大似然估计？<br>
https://www.bilibili.com/video/BV134411u7AS?from=search&seid=15451045870661133064<br>
某事件最可能发生时，反求假设该事件发生时对应的参数<br>


（12.）怎么衡量两个分布的差异？KL散度和交叉熵损失有什么不同？关系是啥？<br>
交叉熵。KL散度更准确的理解是衡量一个分布相比另一个分布的信息损失，不具有交换性。在原有概率分布 p 上，加入我们的近似概率分布 q，计算他们的每个取值对应对数的差sum(pi(logpi-logqi))。<br>


（13.）为什么LR更适合离散特征。<br>
稀疏向量内积乘法运算速度更快。LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合。特征离散后对异常值更具鲁棒性，模型更稳定。<br>











